
<title> Database Design </title>

  *  <a href=#context>Component Context</a>
  *  <a href=#overview>Design Overview</a>
  *  <a href=#btree>B-Tree Structure Overview</a>
  *  <a href=#leaves>B-Tree Leaf Node Operations</a>
  *  <a href=#interior>B-Tree Interior Node Operations</a>

<h1 id=context> Component Context </h1>

This page describes the data structures and algorithms used to implement
the on-disk part of the database system.

The database contains one or more tree structures identified by root page
number, each of which is either an intkey or index tree structure, as required
by SQLite. The special root page number 1 identifies the sqlite3_schema
table.

By the time a client begins writing entries to the database, the transaction
has been reduced to a list of keys and delete-keys that must be written into
the various tree structures within the db. It has already been determined that
the transaction will be committed, and that it does not conflict with any other
concurrently committing transaction. There is no chance rollback will be
required. Any number of non-conflicting transactions may be concurrently
writing to the database.

Each key and delete-key written to the database has an associated 
transaction id. Each reader carries with it a record of the transactions
that were completely written (committed) to the db when its read transaction
started. Keys or delete-keys with transaction ids for transactions that were
not committed when the read transaction was opened are ignored. This means that
it is not a problem if a reader reads a page that contains keys written since
its transaction was opened, so long as it never reads a version of a page too
old to contain keys that are part of transactions that were already completely
committed. There may be any number of readers.

The remainder of this page describes how multiple readers and writers 
proceed concurrently without resorting to locks.

<h1 id=overview> Design Overview </h1>

The distinguishing feature of the database format is the <b>page-map</b>. The
page-map is a massive array large enough to contain an entry for each
logical page in the database. It's primary purpose is to map between
logical and physical page numbers (locations in the database file). In order
to read the data from a page from disk given a logical page number, a client
must first look up the physical page number within the page-map.

The page-map is stored as part of the database file itself and memory mapped
into the database process address space for client access. Clients always 
write to the page-map using atomic compare-and-swap (CAS) instructions.

This arrangements means that clients effectively have a <b>CAS primitive for
entire pages</b>. In order to safely update an existing page, a client:

  #  Reads the current value from the page-map, and then the data from
     the physical page on disk.
  #  Allocates an entirely new physical page and writes the new version
     of the page data into it.
  #  Attempts to modify the page-map entry for the logical page number to
     point to the new physical page on disk using a CAS instruction. If
     the page has been written since step 1, this will fail (as the
     page-map entry will have been modified). 

For the purposes of most of this page, we assume that logical and physical
page numbers are never reused. In practice they must be, because both are
32-bit unsigned integers, and also to avoid the database file growing
indefinitely.

Page-map slots are 64-bits in size, but physical page numbers are just 
32-bits. 1 of the remaining bits is used to store the <b>PAGE_EVICTED flag</b>.
The PAGE_EVICTED is set on a logical page as part of removing it from the
database (see below). Because it is part of the page-map entry, setting
the PAGE_EVICTED flag and the page data may both be considered part of the
thing that is Compared-And-Swapped by the method above.


<h1 id=btree> B-Tree Data Structure </h1>

The tree data structure is similar to a "b-link tree" - a b-tree where each
node has a pointer to its right-hand peer, if any. In some variants, including
this one, some parent-child pointers may at times be missing from the tree
structure. Readers use the peer-peer pointers to navigate to child pages for
which the parent-child pointer is missing.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5 ; linewid=0.2

A1: box ; arrow ; A2: box ; arrow ; A3: box ; arrow ; A4: box ; arrow
A5: box ; arrow ; A6: box ; arrow ; A7: box ; arrow ; A8: box
B1: box at A3 + (0.35, 0.5) ; arrow <- ; B2: box ; arrow <- ; B3: box
C1: box at B2 + (0,0.5) 

arrow from B1.s to A1.n
arrow from B1.s to A2.n
arrow from B2.s to A4.n
arrow from B2.s to A5.n
arrow from B3.s to A6.n
arrow from B3.s to A7.n
arrow from C1.s to B1.n
arrow from C1.s to B3.n
</verbatim>

The first key on a page - either a leaf or internal node - is called the
<b>fencepost key</b>. Once a page has been written to disk, its fencepost key
never changes and the page will never contain a key smaller than the fencepost.
The logical page number and the fencepost key value are indelibly coupled for
all time. Even if the database entry itself is removed, the fencepost key is
left on the page. The only exception is the leftmost peer node at each level of
the tree structure - the fencepost key for this node is implicitly the smallest
possible value in the keyspace.

For both intkey and index trees, all entries are stored on leaf nodes.

Each child pointer in a parent node is accompanied by a copy of the fencepost
key for the child node. This is different from the usual b-tree approach of N
pointers and N-1 keys per internal node page - in this system an internal
node with N pointer contains N keys.

Another difference is that the key sorting order is reversed for each tier
of the tree structure. On nodes that are parents of leaf nodes, keys are 
sorted in reverse order. For nodes that are grandparents of the leaf nodes,
keys sorting is in the same order as on leaves. For great-grandparents,
reversed. And so on. The advantages of this are <a href=#interior>described 
below</a>.

<h1 id=leaves> B-Tree Leaf Node Operations </h1>

The data structure is best analyzed as a series of loosely coupled
linked-lists. Consider, to begin with, just the leaves of a b-tree 
structure - logical pages 1, 2, 3 and 4. The linked-list is being
updated, possibly by multiple concurrent writers, and is also being
traversed from start to finish by multiple readers.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

box "1" ; arrow ; box "2" ; arrow ; box "3" ; arrow ; box "4" ;
</verbatim>

Writing a new key to such a list when the destination page has enough
room for the new entry is easy. The writer locates the page to update,
and uses the <a href=#overview>CAS primitive</a> to write the new version
containing the new key.

If another writer is trying to simultaneously update the same page,
one of the writes will fail. The failed writer restarts its operation to 
write its key. Such a write does not interfere with readers either. Readers
either see the old or new version of the page, and both contain the 
data they require. If the reader sees the new version of the page, it 
ignores the new key based on its transaction id.

There are three slightly more complicated cases:

  #  Splitting a linked-list page in two,
  #  Consolidating two linked-list pages into one, and
  #  Redistributing keys between two linked-list pages.

If there is insufficient space within a node to accommodate a new key, the
node may be <b>split in two</b>. This is a two step procedure:

  #  A new node is allocated and populated with the keys destined for the
     right-hand node of the new pair. The peer node pointer of the new
     node is a copy of the peer node pointer of the node being split. Node 
     5 in the diagram below.
  #  A new version of the node being split (node 1 in the diagram below)
     is written containing with the keys destined for the left-hand node
     of the new pair. The peer node pointer of the new version points to
     the node allocated in the previous step.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; TWO: box "2" ; arrow ; box "3" ; arrow ; box "4" ;
text "Step 1:" with e at ONE + (-0.5, 0)
FIVE: box at TWO + (-0.5, -0.3) "5" color red
arrow from FIVE.e to TWO.s color red
</verbatim>

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" color red ; move ; TWO: box "2" ; arrow ; box "3" ; arrow ; box "4"
text "Step 2:" with e at ONE + (-0.5, 0)

FIVE: box at TWO + (-0.5, -0.3) "5"
arrow from FIVE.e to TWO.s
arrow from ONE.s to FIVE.w color red
</verbatim>
<center><i>Splitting a node in two</i></center>

The new version of the page written in step 2 is of course written using
the CAS primitive. If this fails because some other writer has written
the page being split since it was read, the writer must reclaim the resources
in step 1 and restart the write operation that required the split.

It's not described above, but data must of course sometimes be removed from
a node. This may result in the node becoming so underfull that the keys from
it and its right-hand peer may be <b>consolidated into a single node</b>. 

The diagram below shows the keys in nodes 2 and 3 being consolidated into a
new version of node 2, removing 3 from the tree. This is a two step procedure:

  #  The PAGE_EVICTED flag is set on the right-hand node of the pair
     being consolidated (node 3 in the diagram). This tells other writers 
     that the page may not be written - it is being removed from the tree.
  #  A new version of page 2 is written, containing all keys from 
     pages 2 and 3, with the peer node set to point to page 4.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; box "2" ; arrow ; THREE: box "3" color red; arrow color red; box "4" 
circle "X" radius 0.05 color red at THREE.ne fill white
text "Step 1:" with e at ONE + (-0.5, 0)
</verbatim>

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; TWO: box "2" color red; move ; THREE: box "3"; arrow; FOUR: box "4" 
circle "X" radius 0.05 at THREE.ne fill white
text "Step 2:" with e at ONE + (-0.5, 0)

arrow from TWO.e right 0.1 then right 0.3 down 0.3 then right until even with FOUR.w then to FOUR.s color red
</verbatim>
<center><i>Consolidating two nodes</i></center>

Once the PAGE_EVICTED flag has been set on a page, it may not be cleared.

Setting the PAGE_EVICTED flag on a node does not count as a lock, as it need
not block another writer. If a writer determines that a new key should be
written to a node with the PAGE_EVICTED flag already set, it may perform step
2 of the procedure above (or steps 2 and 3 of the one below) to correct the
problem before restarting its write operation. It does not have to wait on the
writer that set the PAGE_EVICTED flag.

The final case is <b>redistributing keys between two peer nodes</b>. This
cannot be done literally, as it would necessitate modifying the fencepost
key of the right-hand peer, which is not allowed. Instead, the right-hand
peer is removed from the tree and a newly allocated node used in its place.
The following figure shows the three step process to redistribute the keys
in nodes 2 and 3:

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; box "2" ; arrow ; THREE: box "3" color red; arrow color red; box "4" 
circle "X" radius 0.05 color red at THREE.ne fill white
text "Step 1:" with e at ONE + (-0.5, 0)
</verbatim>
<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; TWO: box "2" ; arrow ; THREE: box "3" ; arrow ; FOUR: box "4" ;
circle "X" radius 0.05 at THREE.ne fill white
text "Step 2:" with e at ONE + (-0.5, 0)
FIVE: box at THREE + (0.0, -0.3) "5" color red
arrow from FIVE.e to FOUR.s color red
</verbatim>
<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; TWO: box "2" color red; move ; THREE: box "3" ; arrow ; FOUR: box "4" ;
circle "X" radius 0.05 at THREE.ne fill white
text "Step 3:" with e at ONE + (-0.5, 0)
FIVE: box at THREE + (0.0, -0.3) "5"
arrow from FIVE.e to FOUR.s
arrow from TWO.e to FIVE.w color red
</verbatim>
<center><i>Redistributing keys between two nodes</i></center>

In the consolidating two nodes case above, it may be that by the time the
writer gets to step 2, it finds that some other writer has added keys to
the left-hand peer (node 2 in the diagram) such that the two nodes now
collectively contain too much data to consolidate. In this case the writer
must change tack and proceed with step 2 of the "redistributing keys" case.

<h1 id=interior> Btree Interior Node Operations </h1>

Linked-lists are fine for scanning through in a forwards direction. But
SQLite also requires support for efficiently (a) iterating backwards and
(b) seeking within a tree structure for a nominated key. Singly linked
lists can be used for both of these operations of course, but inefficiently.
For efficient operation, b-tree interior nodes are required.

Each tier of b-tree interior nodes (parents of leaves, grandparents of leaves,
etc.) may be thought of as a separate linked list data structure, just as
the leaf nodes were in the previous section. Each tier is populated with the
fencepost keys and associated logical page numbers from its child pages. After
a writer splits a leaf node into two nodes, it adds the new fencepost key/page
number entry to the parent linked-list. After it marks a leaf page with the
PAGE_EVICTED flag, it removes the corresponding entry from the parent
linked-list. There is no real hurry to perform either of these operations, 
they may be deferred in order to consolidate multiple writes to the same page
of the parent linked-list.

Obviously, this means that when seeking for a key within a tree structure, a
reader may encounter a parent linked-list that does not reflect the latest
update to the child list. Specifically:

<ol>
<li> the parent list may contain entries for pages that have been removed from
     the child list, and
<li> the parent list may be missing entries for pages that have been added
     to the child list.
</ol>

Readers avoid these hazards as follows:

<ol>
<li> a reader never follows a child pointer if the destination logical page
     has already been marked as PAGE_EVICTED. Instead, it moves to the previous
     entry in the parent list and follows that pointer instead. If it turns
     out that the PAGE_EVICTED child actually is still part of the child
     list, and the pointer should have been followed, this transforms this
     case into the following one (missing entry in parent page).

<li> when searching within a page, if the key being seeked to is greater than
     all keys on the page, the reader loads the right-hand peer node and
     determines based on the fencepost key whether or not the peer node
     should be searched as well. This way, if the pointer for the peer node 
     is missing from the parent page, it is searched anyway.
</ol>

In the worst case, a parent list might contain no child pointers not marked
PAGE_EVICTED except for the pointer to the leftmost page of the child
linked-list. In this case performance falls back to the linked-list case.
Otherwise, even if the parent list contains only a few non-PAGE_EVICTED keys,
they may be used to jump to a better position within the child list at 
which to start the search. Since the fencepost key of a logical page never
changes, it is not possible for a non-PAGE_EVICTED entry within a parent
list to be invalid or out of date.

Another quirk is that sorting order is reversed for each successive tier
of the b-tree. That is to say, keys in each parent list are sorted in the
opposite order to those in its child list. This is for two reasons:

  *  If a seek operation would otherwise follow a pointer to a page marked
     PAGE_EVICTED, the reader needs to follow the pointer associated with
     the previous entry to the child list. But iterating backwards can be
     tricky - if the PAGE_EVICTED key is the first on its page, this 
     essentially means retraversing parent nodes. But if the parent list
     is in reverse order, the pointer to follow is the that associated with
     the next entry, which is simple in all cases.

  *  Efficiently iterating backwards through leaf nodes while accounting for
     possible writers is complicated, requiring extra memory and extra seek
     operations within the tree. It is made simpler, as well as more efficient,
     if the parent list is in reverse order.

The following diagram shows a tree structure containing keys from a keyspace
consisting of 8-bit numbers, rendered in hexadecimal. The largest and smallest
possible keys, respectively, are FF and 00. The middle tier of the b-tree
is sorted in the opposite order than the leaf or root tier.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.6 ; linewid=0.2

A1:  box "09  13  19" ; arrow
A2:  box "21  2A  2A" ; arrow
A3:  box "38  3C  43" ; arrow
A4:  box "45  4F  50" ; arrow
A5:  box "51  5E  69" ; arrow
A6:  box "7A  BD  C2" ; arrow
A7:  box "CE  E0  F6" ; arrow
A8:  box "F7  F7  F8"

B1: box "F7  CE  7A" with c at A3 + (0.35, 0.5) ; arrow
B2: box "51  45" ; arrow
B3: box "38  21  00"

C:  box "38  51  FF" with c at B2 + (0, 0.5)

arrow from C.s to B2.n
arrow from C.s left 0.2 then to B3.n
arrow from C.s right 0.2 then to B1.n

arrow from B1.s left 0.2 then to A8.n
arrow from B1.s to A7.n
arrow from B1.s right 0.2 then to A6.n

arrow from B2.s left 0.1 then to A5.n
arrow from B2.s right 0.1 then to A4.n

arrow from B3.s left 0.2 then to A3.n
arrow from B3.s to A2.n
arrow from B3.s right 0.2 then to A1.n
</verbatim>

Alternative rendering of the same tree. Rendered this way, the fencepost
keys for each node in the middle tier of the tree structure appear on the
right-hand-side of each box.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.6 ; linewid=0.2

A1:  box "09  13  19" ; arrow
A2:  box "21  2A  2A" ; arrow
A3:  box "38  3C  43" ; arrow
A4:  box "45  4F  50" ; arrow
A5:  box "51  5E  69" ; arrow
A6:  box "7A  BD  C2" ; arrow
A7:  box "CE  E0  F6" ; arrow
A8:  box "F7  F7  F8"

B1: box "00  21  38" with c at A3 + (0.35, 0.5) ; arrow <-
B2: box "45  51" ; arrow <-
B3: box "7A  CE  F7"

C:  box "38  51  FF" with c at B2 + (0, 0.5)

arrow from C.s to B2.n
arrow from C.s left 0.2 then to B1.n
arrow from C.s right 0.2 then to B3.n

arrow from B1.s left 0.2 then to A1.n
arrow from B1.s to A2.n
arrow from B1.s right 0.2 then to A3.n

arrow from B2.s left 0.1 then to A4.n
arrow from B2.s right 0.1 then to A5.n

arrow from B3.s left 0.2 then to A6.n
arrow from B3.s to A7.n
arrow from B3.s right 0.2 then to A8.n
</verbatim>


