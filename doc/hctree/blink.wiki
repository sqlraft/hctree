
<title> Database Design </title>

  *  <a href=#context>Component Context</a>
  *  <a href=#overview>Design Overview</a>
  *  <a href=#btree>B-Tree Data Structure</a>
    <ul>
      <li><a href=#leaves>B-Tree Leaf Node Operations</a>
      <li><a href=#interior>B-Tree Interior Node Operations</a>
      <li><a href=#other> Changing the Height of the Tree</a>
    </ul>
  *  <a href=#pages> Page Management</a>
  *  <a href=#mvcc> MVCC Version Management </a>
  *  <a href=#recovery> Crash Recovery </a>

<h1 id=context> Component Context </h1>

This page describes the data structures and algorithms used to implement
the on-disk part of the database system.

The database contains one or more tree structures identified by root page
number, each of which is either an intkey or index tree structure, as required
by SQLite. The special root page number 1 identifies the sqlite3_schema
table.

By the time a client begins writing entries to the database, the transaction
has been reduced to a list of keys and delete-keys that must be written into
the various tree structures within the db. It has already been determined that
the transaction will be committed, and that it does not conflict with any other
concurrently committing transaction. There is no chance rollback will be
required. Any number of non-conflicting transactions may be concurrently
writing to the database.

Each key and delete-key written to the database has an associated 
transaction id. Each reader carries with it a record of the transactions
that were completely written (committed) to the db when its read transaction
started. Keys or delete-keys with transaction ids for transactions that were
not committed when the read transaction was opened are ignored. This means that
it is not a problem if a reader reads a page that contains keys written since
its transaction was opened, so long as it never reads a version of a page too
old to contain keys that are part of transactions that were already completely
committed. There may be any number of readers.

The remainder of this page describes the database design at a high level. 
There is also a <a href=fileformat.wiki>file format document</a>.

<h1 id=overview> Design Overview </h1>

The distinguishing feature of the database format is the <b>page-map</b>. The
page-map is a very large array - large enough to contain an entry for each
page in the database. It's primary purpose is to map between logical and
physical page numbers (locations in the database file). In order to read the
data from a page from disk given a logical page number, a client must first
look up the physical page number within the page-map.

The page-map is stored on disk and memory mapped into the database process
address space for client access. Clients always write to the page-map using
atomic compare-and-swap (CAS) instructions.

This arrangement means that clients effectively have a <b>CAS primitive for
entire pages</b>. In order to safely update an existing page, a client:

  #  Reads the current value from the page-map, and then the data from
     the physical page on disk.
  #  Allocates an entirely new physical page and writes the new version
     of the page data into it.
  #  Attempts to modify the page-map entry for the logical page number to
     point to the new physical page on disk using a CAS instruction. If
     the page has been written since step 1, this will fail (as the
     page-map entry will have been modified). 

For the purposes of most of this document, we assume that logical and physical
page numbers are never reused. In practice they must be, partly because both
are 32-bit unsigned integers, and also to avoid the database file growing
indefinitely.

Page-map slots are 64-bits in size, but physical page numbers are just 
32-bits. 1 of the remaining bits is used to store the <b>PAGE_EVICTED flag</b>.
The PAGE_EVICTED is set on a logical page as part of removing it from a 
tree structure (see below). Because it is part of the page-map entry, setting
the PAGE_EVICTED flag and the page data may both be considered part of the
thing that is Compared-And-Swapped by the method above.


<h1 id=btree> B-Tree Data Structure </h1>

The tree data structure is similar to a "b-link tree" - a b-tree where each
node has a pointer to its right-hand peer, if any. In some variants, including
this one, some parent-child pointers may at times be missing from the tree
structure. Readers use the peer-peer pointers to navigate to child pages for
which the parent-child pointer is missing.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5 ; linewid=0.2

A1: box ; arrow ; A2: box ; arrow ; A3: box ; arrow ; A4: box ; arrow
A5: box ; arrow ; A6: box ; arrow ; A7: box ; arrow ; A8: box
B1: box at A3 + (0.35, 0.5) ; arrow <- ; B2: box ; arrow <- ; B3: box
C1: box at B2 + (0,0.5) 

arrow from B1.s to A1.n
arrow from B1.s to A2.n
arrow from B2.s to A4.n
arrow from B2.s to A5.n
arrow from B3.s to A6.n
arrow from B3.s to A7.n
arrow from C1.s to B1.n
arrow from C1.s to B3.n
</verbatim>

The first key on a page - either a leaf or internal node - is called the
<b>fencepost key</b>. Once a page has been written to disk, its fencepost key
never changes and the page will never contain a key smaller than the fencepost.
The logical page number and the fencepost key value are indelibly coupled for
all time. Even if the database entry itself is removed, the fencepost key is
left on the page. The only exception is the leftmost peer node at each level of
the tree structure - the fencepost key for this node is implicitly the smallest
possible value in the keyspace.

For both intkey and index trees, all entries are stored on leaf nodes.

Each child pointer in a parent node is accompanied by a copy of the fencepost
key for the child node. This is different from the usual b-tree approach of N
pointers and N-1 keys per internal node page - in this system an internal
node with N pointer contains N keys.

Another departure from conventionality is that the key sorting order is
reversed for each tier of the tree structure. On nodes that are parents of leaf
nodes, keys are sorted in reverse order. For nodes that are grandparents of the
leaf nodes, keys sorting is in the same order as on leaves. For
great-grandparents, reversed. And so on. The advantages of this are <a
href=#interior>described below</a>.

<h2 id=leaves> B-Tree Leaf Node Operations </h2>

The data structure is best analyzed as a series of loosely coupled
linked-lists. Consider, to begin with, just the leaves of a b-tree 
structure - logical pages 1, 2, 3 and 4. The linked-list is being
updated, possibly by multiple concurrent writers, and is also being
traversed from start to finish by multiple readers.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

box "1" ; arrow ; box "2" ; arrow ; box "3" ; arrow ; box "4" ;
</verbatim>

Writing a new key to such a list when the destination page has enough
room for the new entry is easy. The writer locates the page to update,
and uses the <a href=#overview>CAS primitive</a> to write the new version
of the page containing the new key.

If another writer is trying to simultaneously update the same page,
one of the writes will fail. The failed writer restarts its operation to 
write its key. Such a write does not interfere with readers either. Readers
either see the old or new version of the page, and both contain the 
data they require. If the reader sees the new version of the page, it 
ignores the new key based on its transaction id.

There are three slightly more complicated cases:

  #  Splitting a linked-list page in two,
  #  Consolidating two linked-list pages into one, and
  #  Redistributing keys between two linked-list pages.

If there is insufficient space within a node to accommodate a new key, the
node may be <b>split in two</b>. This is a two step procedure:

  #  A new node is allocated and populated with the keys destined for the
     right-hand node of the new pair. The peer node pointer of the new
     node is a copy of the peer node pointer of the node being split. Node 
     5 in the diagram below.
  #  A new version of the node being split (node 1 in the diagram below)
     is written containing with the keys destined for the left-hand node
     of the new pair. The peer node pointer of the new version points to
     the node allocated in the previous step.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; TWO: box "2" ; arrow ; box "3" ; arrow ; box "4" ;
text "Step 1:" with e at ONE + (-0.5, 0)
FIVE: box at TWO + (-0.5, -0.3) "5" color red
arrow from FIVE.e to TWO.s color red
</verbatim>

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" color red ; move ; TWO: box "2" ; arrow ; box "3" ; arrow ; box "4"
text "Step 2:" with e at ONE + (-0.5, 0)

FIVE: box at TWO + (-0.5, -0.3) "5"
arrow from FIVE.e to TWO.s
arrow from ONE.s to FIVE.w color red
</verbatim>
<center><i>Splitting a node in two</i></center>

The new version of the page written in step 2 is of course written using
the CAS primitive. If this fails because some other writer has written
the page being split since it was read, the writer must reclaim the resources
in step 1 and restart the write operation that required the split.

It's not described above, but data must of course sometimes be removed from
a node. This may result in the node becoming so underfull that the keys from
it and its right-hand peer may be <b>consolidated into a single node</b>. 

The diagram below shows the keys in nodes 2 and 3 being consolidated into a
new version of node 2, removing 3 from the tree. This is a two step procedure:

  #  The PAGE_EVICTED flag is set on the right-hand node of the pair
     being consolidated (node 3 in the diagram). This tells other writers 
     that the page may not be written - it is being removed from the tree.
  #  A new version of page 2 is written, containing all keys from 
     pages 2 and 3, with the peer node set to point to page 4.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; box "2" ; arrow ; THREE: box "3" color red; arrow color red; box "4" 
circle "X" radius 0.05 color red at THREE.ne fill white
text "Step 1:" with e at ONE + (-0.5, 0)
</verbatim>

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5

ONE: box "1" ; arrow ; TWO: box "2" color red; move ; THREE: box "3"; arrow; FOUR: box "4" 
circle "X" radius 0.05 at THREE.ne fill white
text "Step 2:" with e at ONE + (-0.5, 0)

arrow from TWO.e right 0.1 then right 0.3 down 0.3 then right until even with FOUR.w then to FOUR.s color red
</verbatim>
<center><i>Consolidating two nodes</i></center>

Once the PAGE_EVICTED flag has been set on a page, it may not be cleared.

Setting the PAGE_EVICTED flag on a node does not count as a lock, as it need
not block another writer. If a writer determines that a new key should be
written to a node with the PAGE_EVICTED flag already set, it may perform step
2 of the procedure above (or steps 2 and 3 of the one below) to correct the
problem before restarting its write operation. It does not have to wait on the
writer that set the PAGE_EVICTED flag.

The final case is <b>redistributing keys between two peer nodes</b>. This
cannot be done literally, as it would necessitate modifying the fencepost
key of the right-hand peer, which is not allowed. Instead, the right-hand
peer is removed from the tree and a newly allocated node used in its place.
The following figure shows the three step process to redistribute the keys
in nodes 2 and 3:

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; box "2" ; arrow ; THREE: box "3" color red; arrow color red; box "4" 
circle "X" radius 0.05 color red at THREE.ne fill white
text "Step 1:" with e at ONE + (-0.5, 0)
</verbatim>
<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; TWO: box "2" ; arrow ; THREE: box "3" ; arrow ; FOUR: box "4" ;
circle "X" radius 0.05 at THREE.ne fill white
text "Step 2:" with e at ONE + (-0.5, 0)
FIVE: box at THREE + (0.0, -0.3) "5" color red
arrow from FIVE.e to FOUR.s color red
</verbatim>
<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
ONE: box "1" ; arrow ; TWO: box "2" color red; move ; THREE: box "3" ; arrow ; FOUR: box "4" ;
circle "X" radius 0.05 at THREE.ne fill white
text "Step 3:" with e at ONE + (-0.5, 0)
FIVE: box at THREE + (0.0, -0.3) "5"
arrow from FIVE.e to FOUR.s
arrow from TWO.e to FIVE.w color red
</verbatim>
<center><i>Redistributing keys between two nodes</i></center>

In the consolidating two nodes case above, it may be that by the time the
writer gets to step 2, it finds that some other writer has added keys to
the left-hand peer (node 2 in the diagram) such that the two nodes now
collectively contain too much data to consolidate. In this case the writer
must change tack and proceed with step 2 of the "redistributing keys" case.

<h2 id=interior> Btree Interior Node Operations </h2>

Linked-lists are fine for scanning through in a forwards direction. But
SQLite also requires support for efficiently (a) iterating backwards and
(b) seeking within a tree structure for a nominated key. Singly linked
lists can be used for both of these operations of course, but inefficiently.
For efficient operation, b-tree interior nodes are required.

Each tier of b-tree interior nodes (parents of leaves, grandparents of leaves,
etc.) may be thought of as a separate linked list data structure, just as
the leaf nodes were in the previous section. Each tier is populated with the
fencepost keys and associated logical page numbers from its child pages. After
a writer splits a leaf node into two nodes, it adds the new fencepost key/page
number entry to the parent linked-list. After it marks a leaf page with the
PAGE_EVICTED flag, it removes the corresponding entry from the parent
linked-list. There is no real hurry to perform either of these operations, 
they may be deferred in order to consolidate multiple writes to the same page
of the parent linked-list.

Obviously, this means that when seeking for a key within a tree structure, a
reader may encounter a parent linked-list that does not reflect the latest
update to the child list. Specifically:

<ol>
<li> the parent list may contain entries for pages that have been removed from
     the child list, and
<li> the parent list may be missing entries for pages that have been added
     to the child list.
</ol>

Readers avoid these hazards as follows:

<ol>
<li> a reader never follows a child pointer if the destination logical page
     has already been marked as PAGE_EVICTED. Instead, it moves to the previous
     entry in the parent list and follows that pointer instead. If it turns
     out that the PAGE_EVICTED child actually is still part of the child
     list, and the pointer should have been followed, this transforms this
     case into the following one (missing entry in parent page).

<li> when searching within a page, if the key being seeked to is greater than
     all keys on the page, the reader loads the right-hand peer node and
     determines based on the fencepost key whether or not the peer node
     should be searched as well. This way, if the pointer for the peer node 
     is missing from the parent page, it is searched anyway.
</ol>

In the worst case, a parent list might contain no child pointers not marked
PAGE_EVICTED except for the pointer to the leftmost page of the child
linked-list. In this case performance falls back to the linked-list case.
Otherwise, even if the parent list contains only a few non-PAGE_EVICTED keys,
they may be used to jump to a better position within the child list at 
which to start the search. Since the fencepost key of a logical page never
changes, it is not possible for a non-PAGE_EVICTED entry within a parent
list to be invalid or out of date.

Another quirk is that sorting order is reversed for each successive tier
of the b-tree. That is to say, keys in each parent list are sorted in the
opposite order to those in its child list. This is for two reasons:

  *  If a seek operation would otherwise follow a pointer to a page marked
     PAGE_EVICTED, the reader needs to follow the pointer associated with
     the previous entry to the child list. But iterating backwards can be
     tricky - if the PAGE_EVICTED key is the first on its page, this 
     essentially means retraversing parent nodes. But if the parent list
     is in reverse order, the pointer to follow is the that associated with
     the next entry, which is simple in all cases.

  *  Efficiently iterating backwards through leaf nodes while accounting for
     possible writers is complicated, requiring extra memory and extra seek
     operations within the tree. It is made simpler, as well as more efficient,
     if the parent list is in reverse order.

The following diagram shows a tree structure containing keys from a keyspace
consisting of 8-bit numbers, rendered in hexadecimal. The largest and smallest
possible keys, respectively, are FF and 00. The middle tier of the b-tree
is sorted in the opposite order than the leaf or root tier.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.6 ; linewid=0.2

A1:  box "09  13  19" ; arrow
A2:  box "21  2A  2A" ; arrow
A3:  box "38  3C  43" ; arrow
A4:  box "45  4F  50" ; arrow
A5:  box "51  5E  69" ; arrow
A6:  box "7A  BD  C2" ; arrow
A7:  box "CE  E0  F6" ; arrow
A8:  box "F7  F7  F8"

B1: box "F7  CE  7A" with c at A3 + (0.35, 0.5) ; arrow
B2: box "51  45" ; arrow
B3: box "38  21  00"

C:  box "38  51  FF" with c at B2 + (0, 0.5)

arrow from C.s to B2.n
arrow from C.s left 0.2 then to B3.n
arrow from C.s right 0.2 then to B1.n

arrow from B1.s left 0.2 then to A8.n
arrow from B1.s to A7.n
arrow from B1.s right 0.2 then to A6.n

arrow from B2.s left 0.1 then to A5.n
arrow from B2.s right 0.1 then to A4.n

arrow from B3.s left 0.2 then to A3.n
arrow from B3.s to A2.n
arrow from B3.s right 0.2 then to A1.n
</verbatim>

Alternative rendering of the same tree. Rendered this way, the fencepost
keys for each node in the middle tier of the tree structure appear on the
right-hand-side of each box.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.6 ; linewid=0.2

A1:  box "09  13  19" ; arrow
A2:  box "21  2A  2A" ; arrow
A3:  box "38  3C  43" ; arrow
A4:  box "45  4F  50" ; arrow
A5:  box "51  5E  69" ; arrow
A6:  box "7A  BD  C2" ; arrow
A7:  box "CE  E0  F6" ; arrow
A8:  box "F7  F7  F8"

B1: box "00  21  38" with c at A3 + (0.35, 0.5) ; arrow <-
B2: box "45  51" ; arrow <-
B3: box "7A  CE  F7"

C:  box "38  51  FF" with c at B2 + (0, 0.5)

arrow from C.s to B2.n
arrow from C.s left 0.2 then to B1.n
arrow from C.s right 0.2 then to B3.n

arrow from B1.s left 0.2 then to A1.n
arrow from B1.s to A2.n
arrow from B1.s right 0.2 then to A3.n

arrow from B2.s left 0.1 then to A4.n
arrow from B2.s right 0.1 then to A5.n

arrow from B3.s left 0.2 then to A6.n
arrow from B3.s to A7.n
arrow from B3.s right 0.2 then to A8.n
</verbatim>

<h2 id=other> Changing the Height of the Tree</h2>

Each tree structure is initially created as a single leaf node that is also
the root of the tree. Once the root node is full, it is not split in two
as any other node would be, but instead:

  #  Two new leaf nodes are allocated and populated with all the keys from
     the root node.
  #  The root node is rewritten so as to be the parent of the two new leaves.

This occurs any time the root node is full, just as it does in a regular
b-tree. A root node is not allowed to have a peer node. Each time this
occurs, the height of the tree has increased by 1.

<verbatim type="pikchr center">
boxht=0.2 ; boxwid=0.5
R1: box "1"
L1: box "2" with c at R1 + (-0.5, -0.5) color red; 
arrow color red; box "3" color red
text "Step 1:" with e at L1 + (-0.5, 0.25)

R2: box "1" with c at R1 + (2.5, 0) color red
L2: box "2" with c at R2 + (-0.5, -0.5); arrow ; L3: box "3"
arrow color red from R2.s left 0.1 then to L2.n
arrow color red from R2.s right 0.1 then to L3.n
text "Step 2:" with e at L2 + (-0.5, 0.25)

</verbatim>

If the root node has only a single child, the height of the tree can be
decreased by 1. This is done as follows:

  #  The child node is marked as PAGE_EVICTED.
  #  The contents of the child node are copied into the parent.

Step 1 of the above is a special case - it is the only case in which the 
first node in a linked-list is marked as PAGE_EVICTED. As a result, special
rules apply. It is still not possible to write to the page once it is marked
as PAGE_EVICTED, but readers are allowed to traverse the parent-child pointer.
This allows readers (and other writers) to continue while the height of the
tree is being decreased.

<h1 id=pages> Page Management</h1>

The description in the sections above assume that there is an infinite supply
of both physical pages and logical page ids, and that those used and no longer
required may be carelessly discarded. Unfortunately, this is not the case.

For both physical and logical pages, we accept that a process failure at
an inopportune moment might cause a resource leak. These are not expected
to amount to much.

<b>Physical Pages</b>

Free physical pages are identified within the database file using the 
following mechanisms:

  *  A header field contains the largest physical page allocated so far.
  *  For each page that is in use, the PHYSICAL_IN_USE flag is set in the
     corresponding page-map slot.

Internally, clients allocate physical pages N (say 128) at a time. To allocate
a set of N physical pages, the client first takes a database-wide mutex. It
assigns itself N new physical page ids either by incrementing the header
field or by removing elements from an in-memory table of eligible free physical
pages.  

When a client needs to write a physical page that has already been allocated 
to it, it:

  #  Sets the PHYSICAL_IN_USE flag in the page-map,
  #  Writes data to the database page.
  #  Attempts to set the page-map entry for the logical page to point to
     the new physical page.
  #  The above may fail if another client has already written to the logical 
     page. If it does, the PHYSICAL_IN_USE flag set in step 1 is cleared
     and the client retains the physical page id for use in subsequent writes.

In order to populate the in-memory table of free physical pages, the page-map
is scanned for entries with clear PHYSICAL_IN_USE flags on system startup.
This is no trivial undertaking - in a database with 4KiB pages, there is 2MB of
page-map for each 1GB of database file. A 16TB database contains a 32GB
page-map. It might be done lazily, allowing clients to proceed by allocating
pages only by incrementing the header file until the scan is complete. 

Pages that are free when the system starts may be written immediately. Pages 
that have been made free since system startup may only be reused once it
is sure that no reader will attempt to access the old version of the page.
In other words, once all readers are reading from a snapshot that includes
the transaction that removed the page from the tree.

When a client writes a new version of a logical page, freeing an old 
physical page id, it clears the PHYSICAL_IN_USE flag in the page-map
immediately. It stores the freed page id and current transaction id
locally in a private table. If the private table grows sufficiently large,
or if the client is closed, it transfers the contents of the private table
to the database-wide in-memory table, under cover of the mutex mentioned
above. The transaction id remains associated with each page id within the
database-wide in-memory table.

Physical pages may be reused either from a clients private table or by
taking the mutex and allocating them from the database-wide table, so long
as it can be determined that there are no readers old enough to be reading
a snapshot that does not include the transaction that freed the page.
<i style=color:green>See elsewhere for how this is determined.</i>

<b>Logical Pages</b>

Logical page id are managed quite similarly to physical page ids. They
are slightly more complicated because a logical page id might be present
in the parent list, making it more difficult to know exactly when a logical
page id becomes free.

To manage this we add another flag - LOGICAL_IN_PARENT - that must be
set if the logical page id is present in the parent list. It may not
be set if PAGE_EVICTED is already set. When a client needs to insert
the entry into a parent list, it:

  #  Sets the LOGICAL_IN_PARENT flag. Or abandon the insert if PAGE_EVICTED
     is already set.
  #  Inserts the new entry into the parent list.

When a page is removed from its list (by writing a new version of its
predecessor in the list, after having set PAGE_EVICTED), its physical
page mapping is set to 0. A page is free if its physical page mapping
is 0 and the LOGICAL_IN_PARENT flag is clear.

When an entry is removed from any list except the list of leaves, a client:

  #  Removes the entry from the list, then
  #  Clears the LOGICAL_IN_PARENT flag.

The client that causes the logical page id to become free, either by clearing
the LOGICAL_IN_PARENT flag or by setting the mapped physical page id to 0
takes ownership of the freed page id and adds it to its private table of
free logical page ids. As with physical page ids, this logical page id
is associated with the current transaction id.

<h1 id=mvcc> MVCC Key Management </h1>

When a new key/value pair is written to the database, the writer associates
with it a transaction id. When a reader reads the page, it inspects the
transaction id of each record and ignores those written by transactions
that are not part of its database snapshot. If the new key/value has
overwritten a previous version, the reader requires access to that previous
version instead of the new one.

To facilitate this, each key/value database entry has associated with it
the physical page number of the last database page to contain the previous
version (if any). As <a href=#pages>described above</a>, this page is
guaranteed not to be reused until all readers are using a snapshot that
includes the transaction that overwrote it, which is the same transaction
id associated with the new key/value database entry (and so the pointer
will never be followed after the page has become eligible for reuse).

Deletes are handled similarly. When an entry is deleted, it is clobbered
by a delete key. So a delete key entry consists of:

  *  The key being deleted,
  *  The transaction id of the transaction that did the deleting, and
  *  The physical page id on which the deleted version of the key can be found.

Whenever a page is rewritten, any delete keys with transaction ids so old that
it is guaranteed that all extant and future readers read from snapshots that
include it may be discarded. As may the transaction id and old page number (if
any) from any other, non-delete, keys.

<b>External Delete-Key Sets</b>

In most scenarios, the above should be sufficient. But in the presence of 
long running read transactions or large deletes, some database pages may come
to contain a high percentage of delete-keys. Which is inefficient for new
readers that do not require them. And, as the delete-keys remain in place 
until at least the next time the page is written, this situation may persist
indefinitely.

The solution is to move delete keys to an <i>external delete-key set</i>. An
external delete-key set is an immutable b-tree structure within the database
file that contains only delete-keys. All pointers to and within the b-tree 
are physical page numbers. When the EDKS is constructed, all pages are 
immediately marked as eligible for reuse once all readers are reading from
a snapshot including transaction T, where T is the largest transaction id 
associated with any delete-key within the data structure. This way, space
used by the entire EDKS and its delete keys is reclaimed "automatically"
by the same mechanism that reuses physical page locations.

Each leaf page in the database may contain pointers to one or more EDKS
b-trees. Each pointer is made up of a physical page id (the root page of
the EDKS b-tree) and the transaction id associated with it ("T" in the 
paragraph above). When a reader using a snapshot that does not include
transaction T reads the page, it must merge in the contents of the 
indicated EDKS. It is not safe for a reader using a snapshot that does
include T to follow the pointer, as the physical pages that make up the EDSK
may already be eligible for reuse.

Multiple leaf pages within the database may contain pointers to the same EDSK.
This may occur, for example, when a leaf page containing an EDSK pointer is
split into two or more pages. Readers use the fencepost keys on the current
leaf and its peer to avoid merging any out-of-range keys from the EDSK into
the page when it is read.

<h1 id=recovery> Crash Recovery </h1>

Recovery from application failure is handled differently from recovery from
power, OS or other system failures (hereafter summarized as "power failures").
In the first case it is assumed that any data written to the file-system before
the crash, either via a system call or a memory mapping, is present and
uncorrupted following the crash. In the latter case, it is assumed that any
disk sector written but not synced when the failure occured may be corrupted.
Following recovery, such a sector may contain the original data, the updated
data, or any other values.

Supporting recovery from power failure requires periodic calls to fsync(), and
some extra processing from writers. Deployments that do not require recovery of
this nature can omit these extra operations.

<h2 id=recovery1> Recovery From Application Failure </h2>

To support application failure, each writer client has a log file. Once
it is known that a transaction does not conflict and is to be written into
the database file, an entry is appended to the log file containing all
the logical write operations that make up the transaction, along with the
transaction id. Once the transaction has been completely written to the
database, the log file entry is marked as committed.

Upon recovery, the database process scans the log files for transactions
not marked as committed. They are then written into the database, one
key at a time, using the usual methods. This avoids partial transactions
being left in the database following an application failure.

<h2 id=recovery2> Recovery From Power/OS Failure </h2>

In order to support recovery from power failure, each database has three
separate page-map files - A, B and C. We refer to "files" in this section, but
these could just as easily be three different parts of a single file, or three
different parts of the database file itself.

A database also has two header pages that occur at the start of the database
file. Each header page is located on a different disk sector, so it can be
synced to disk separately.

Robustness in the face of power failures is achieved using database checkpoints
and checksummed log files. A checkpoint is a version of the page-map and a set
of pages within the database file that have been successfully synced to disk. A
checkpoint contains all data for all transactions with transaction ids less
than or equal to some value T, and no data for any transaction with a
transaction greater than T. To recover the database following a power failure,
as many transactions as possible are recovered from the checksummed log files
and added to the checkpointed version of the database.

Writers begin by writing transactions into the database and into page-map 
file A. Each writer also writes its transactions into a log file (one log 
file per writer), along with a checksum and transaction id for each. To
checkpoint a version of the database:

  *  the "next" page-map file (file B in this case) is populated entirely
     with NULL entries.

  *  new transactions are prevented from beginning to commit until all
     currently committing transactions have finished committing. The 
     checkpoint will contain all transactions up to and including the
     transaction with the largest transaction id committed at this point.

  *  when new transactions are permitted to begin committing again, new
     values are written into page-map file B. When a value is read, an
     attempt is first made to read it from file B. If file B contains a
     NULL value, the real value is read from file A and copied into file B
     using a CAS instruction. If the write fails, the value is reread
     from file B.

  *  The database file is synced to disk.

  *  Page-map file A is synced to disk.

  *  A header page in the database file is updated to indicate that page-map
     file A now correspond to the latest checkpointed version of the database
     file and the transaction id of the checkpointed snapshot. The database
     file is synced to disk. Successive writes alternate back and forth 
     between the two header pages.

  *  The checkpointer thread loops through the entire page-map copying
     values from file A to file B. File B is always written using the CAS
     primitive, so there is no chance of accidentally clobbering a value
     written by a writer following the transaction barrier.

For this to work, writers must only recycle a physical page made eligible
for recycling before the most recent completed checkpoint. Logical page
ids may still be recycled as soon as they are eligible.

<i style=color:green>Should the two "header pages" be moved to a separate
file? So that writing and syncing them does not imply syncing whatever has
been written to the database file in the meantime.</i>





