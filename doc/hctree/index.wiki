

  *  [#system_overview|System Overview]
  *  [#transaction    |Transaction Layer Details]
  *  [#btree          |Btree Layer Details]
    <ul> <li> [#pagemap          |Page-Mapping Array]
   </ul>
  *  [#replication    |Replication and other Integration Issues]
  *  [#robustness     |Process, Power and Operating System Failures]
  *  [#embedded       |Embedded Deployment]

A high-concurrency, optimistic row-level-locking backend for SQLite that
more or less plugs in to the btree.h interface. All clients must be within a
single process. Databases are robust in the face of process crashes.

The database may also be configured to be robust in the face of operating
system crashes and power failures. Recovering from such a crash requires 
scanning the entire database file. [#robustness|See below] for 
details.

Suitable for use in a server system such as BedrockDB.

There is also a [./fileformat.wiki | file format document].

<h1 id=system_overview>System Overview</h1>

The system is described here in two parts - the b-tree layer and the 
transaction layer. The b-tree layer differs from the SQLite b-tree 
in that:

  *  No direct support for transactions, sub-transactions or transaction
     rollback is provided. Each key operation (insert/update/delete) is
     immediately visible to all other database users.
  *  Concurrent writers are supported.

MVCC snapshots for readers are supported using the same technique as Postgres:

  *  Appended to each key inserted into the b-tree is a transaction id.
  *  Each key has an optional field for the transaction id of the transaction
     that deletes it from the database.
  *  Transaction ids are 56-bit monotonically increasing integers.
  *  To read the newest available stable snapshot, readers need to know the 
     largest committed transaction id for which all previous transactions
     are completely committed. They may then selectively ignore database
     entries based on the created and deleted transaction ids associated with
     each key.
  *  Writers remove entries and transaction id fields that are no longer
     required lazily, when the page is updated for some other reason.
     To do this they need to know the oldest transaction id that may
     still be being used by a reader as a snapshot id as described in
     the previous bullet point.

A writer does not write directly to the b-tree structures when executing
DML/DDL statements. Instead, new keys are accumulated in private memory
structures until the transaction is ready to commit. Also accumulated are
a list of keys and key-ranges read from the database as part of the
transaction.

The transaction queue contains an entry for each transactions that has
either been committed, or is being committed, sorted in order of transaction
id. A transaction is not added to the transaction queue or assigned a
transaction id until it is certain that it will be committed - that it does 
not conflict with any other transaction. Associated with each transaction
queue entry is the set of database keys that the transaction writes to.

When a writer wishes to commit a transaction it iterates through the 
transaction queue, starting with the entry with a transaction id
one greater than the snapshot id it used while reading the database,
in ascending transaction id order, checking for conflicts. A conflict
is found if the writer's transaction reads a key or range of keys that
was modified by a transaction already in the queue. If the writer
reaches the head of the transaction queue without finding a conflict,
it adds its own transaction to the end of the queue and in doing so
assigns itself a transaction id. It then writes its keys to the database.

<h1 id=transaction>Transaction Layer Details</h1>

This section describes roughly the transaction layer - the layer that sits
between SQLite and the b-tree described in the sections below. In general
the data structures described here require a mutex to modify, but may be
read safely without one. There are two mutexes for each database:

  *  M1: Mutex taken when adding an entry to the end of the transaction queue.
  *  M2: Mutex taken when a transaction has finished committing (after all
     keys have been written to the b-tree layer).

<b> Readers: </b>

There is a single 64-bit integer accessed using atomic store/get instructions
that contains the newest available snapshot id (the transaction id for which
all previous transactions have been completely committed). There are also two
64-bit locking-slots (or arrays of locking slots). A locking-slot is similar to
the locking slots used by SQLite's wal mode, except that in this case the lower
56-bits of the slot are used for the snapshot id and the remaining 8 for the
number of clients currently holding a lock on the slot. And so a single CAS
instruction is used to take or release a lock. The value of the newest
available snapshot field, as well as the transaction ids stored in the
locking-slots, are set by writers holding the M2 mutex.

More specifically, to open a transaction (read or write), a client must:

  1.  Read the value of both locking-slots.
  2.  Use a CAS instruction to increment the lock-count of the locking-slot
      containing the largest snapshot id. If this fails, goto 1.
  3.  Read the value of the "newest snapshot" field. This should always be
      equal to or greater than the locking-slot snapshot id.

It may be that with a sufficiently high number of clients opening and closing
transactions, contention when writing the two locking-slots might slow things
down. Or 256 or more clients require a lock on the same slot. In this case each
locking-slot will be replaced with an array of locking-slots, spaced out in
memory to avoid using the same cache-line for any two.

<b> Writers: </b>

Each writer has its own log file, and all log files are stored in a single
directory. After a writer assigns a transaction id to a transaction (once
it is sure to be committed), a record of all keys in the transaction, along
with the transaction id, is written to the log file. Once the transaction
has been completely committed, the record within the log file is marked
as invalid. 

These per-writer logs are used for process crash recovery only. On startup, 
all logs in the log-directory are scanned for transactions that were partially
committed when the crash occurred. The transactions are played forward and
the log files deleted as part of startup.

The transaction queue, the list of transactions in order of transaction
id, is an ordinary singly linked-list in heap-memory. Associated with each
of the locking-slot arrays is a pointer to the corresponding element of
the transaction queue (i.e. the element with a transaction id identical
to the locking-slots snapshot id). When a writer needs to iterate through
the transaction queue, it always starts with the pointer associated with
its locking slot.

A writer therefore proceeds as follows:

  #  Opens a read transaction as described above. During the transaction
     new keys and key-deletes are stored in memory, not written into the
     database. Also recorded are the ranges of keys read from the database
     by the transaction.
  #  Iterate through the transaction queue, checking if the current
     transaction conflicts with a transaction with an id greater than
     the snapshot id opened in (1). If a conflict is found, return the 
     error to the user.
  #  Grab mutex M1. Append an entry for the current transaction to the
     transaction queue and assign the next transaction id to it. Release
     M1. Or, if when M1 is obtained some transaction that was not checked
     for conflicts in (2) has already been appended to the list, drop mutex
     M1 and goto 2.
  #  Write an entry for the transaction to the writers log file.
  #  Write all the transactions key and delete-keys to the database.
  #  Grab mutex M2 and mark the transaction queue entry as fully committed
     and update the newest-available-snapshot field if required. Also,
     if possible, update the locking-slot values and the transaction queue
     pointers (see below).

The transaction layer data structure can be summarized as follows:

<verbatim>
    struct TransactionLayer {
      Mutex M1;
      Mutex M2;
      u64 aLockingSlot1[ARRAY_SIZE];
      u64 aLockingSlot2[ARRAY_SIZE];
      u64 iNewestAvailableSnapshot;
      TransactionQueueEntry *p1;
      TransactionQueueEntry *p2;
    };
</verbatim>

Not shown above is the "largest transaction id allocated so far" field. This
is part of the header of the database itself.

After obtaining mutex M2 once all keys have been written to the database,
a writer check if it is possible to update a locking-slot value and
transaction queue pointer. It is always the locking-slot with the smaller
value that is updated. The update takes place iff (a) there are no readers
holding any locks on the locking-slot, and (b) the newest-available-snapshot-id
is greater than the value stored in the other locking slot (as it is important
that the locking slots always contain different values). The update proceeds as
follows:

  #  Each value in the locking-slot array is set to 0. This ensures that no
     reader locks it.
  #  The transaction-queue pointer is advanced to point to the entry with
     a transaction id equal to the current newest-available-snapshot-id
     value. All transaction queue entries between the current pointer and
     the pointer associated with the other locking slot can be deleted
     at this point.
  #  Each value in the locking-slot array is set to the
     newest-available-snapshot-id value.



<h1 id=btree>Btree Layer Details </h1>

The b-tree is an ordinary single-process, multi-threaded, COW, (mostly)
lock-free b-tree implementation. As with SQLite, multiple tree structures 
exist within a single file - one for each database table and index.

<h2 id=pagemap>Page-Mapping Array</h2>

The key feature is a vast array mapping from logical to physical page numbers.
Logical page numbers are those used within the b-tree structure as pointers to
other nodes, physical page numbers are multiplied by the page-size to find the
offset of the page data within the file. To follow a pointer "pgno" just read
from a b-tree page:

<verbatim>
  offset = pgno * pageSize;                             // SQLite
  offset = aPageMap[pgno] * pageSize;                   // Proposed system
</verbatim>

Both logical and physical page numbers are unsigned 32-bit values, so they
are finite resources. For the purposes of this section and the next though, it
is assumed that they are infinite. 

Each slot in the array is 64-bits in size. 32-bits are used by the physical
page number, and several other bits are used as flags. This means that for a 
db with 4KiB pages, this array will be roughly 0.2% of the size of the 
database - 2MB of mapping for each 1GB of database.

The array is stored within the database file in blocks of some configurable
size (say 1MB). i.e. 1MB of array space followed by 128K database pages. The
blocks of the array are memory mapped by the client process.

Because an array entry can be safely modified using a 64-bit CAS instruction,
the system effectively has a CAS primitive for single database pages. To
overwrite a logical database page:

<ol>
  <li> A new physical page number (location in the db file) is allocated.
  <li> The newly allocated page is populated with the new data for the page.
  <li> The array entry is set to the new physical page number using a CAS 
       instruction. If the logical database page has not been modified since
       it was read, the array entry is successfully set and the logical page
       updated. Or, if it has already been updated, the attempt fails. 
</ol>

This works because a new physical page number is used each time a logical page
is overwritten. And physical page numbers are never reused (in practice - are
never reused within the lifetime of a single SQL level transaction).

Additionally, a few of the extra bits in each array slot are flags associated
with the logical page. Because these are part of the same 64-bit value as
the physical page number, they may be thought of as part of the page data -
they are updated and read atomically along with it. Except that the flags
may be read or updated much more cheaply than any other data on the page.

<h2>B-Tree Structures</h2>

The tree structures used are b-link trees. This means that each node has a
pointer to its right-hand peer, if any. So all leaf nodes are linked into 
one singly-linked list, and all nodes that are parents of leaves into another,
and so on.

Simple writes to leaf pages - those that do not change the structure of the
b-tree - do not interfere with readers at all. Readers either see the first
version of the page (and therefore page), or the second. Either is valid.

Operations that modify the tree structure are more complicated. They require
careful coordination of both readers and writers. Instead of latching or
locking multiple nodes in order to perform the usual operations to rebalance a
b-tree, the operations are modified slightly so that they consist of a series
of single-page modifications, each of which leaves the database is a valid
state. Simpler node-splits and two-node merges are used instead of the more
complicated 3 or 4 sibling balancing operations used by SQLite.

<center>
<img src=btree_split1.png>
<p><i>Write and split of leaf node 3 using a series of atomic page writes</i>
</center>

The figure above demonstrates the atomic operations that make up a node-split.
In this case an attempt was made to insert a new key into node 3, but node 3
is already full. The node id values used here are of course synonymous with
logical page numbers. The figure shows:

  #  <p>The initial state of the b-tree structure. An attempt is made to add
     a new key to node 3, but node 3 is already full.
  #  <p>Instead, a new node, node 5 is allocated. Node 5 is populated to
     contain half of the keys from node 3 (including, possibly, the new key).
     Its sibling pointer is set to point to node 4. But it is not yet linked
     into the tree structure anywhere - no reader can know that it exists.
  #  <p>Node 3 is modified so that it only contains those keys not transfered
     to node 5. And so that its sibling pointer points to new node 5. The tree
     is now in an unorthodox state - node 3 is now a "super-node". A super-node
     is a linked-list of 2 nodes, in this case 3 and 5, that must be treated
     as a single node by readers. A super-node is identified by the fact
     that its sibling pointer does not match the sibling in the parent node (in
     this case node 4).
  #  <p>Node 1 is updated to include a new split-key and a pointer to node 5,
     restoring b-tree orthordoxy and demoting node 3 back to a non-super node.

When it is created, node 5 contains the divider key that will eventually
be inserted into node 1. In the case of leaf nodes, this is just a copy of
the smallest key on the page. For internal nodes, it is a distinct key. New 
keys may be written into either half of the super-node while it is still a
super-node, but the divider key may not be modified. Keys smaller than the
divider key must be inserted into the LHS of the super-node, keys larger must
be inserted into the RHS. Nor can either half of a super-node be itself
split into a super node - the pointer in the parent node must be added first.

The procedure for inserting into a leaf node is repeated recursively up
the tree as required. In step 4 above, a new key is inserted into parent
node 1. If there is not enough room for the new key in node 1, it is split
into a super-node.

<center>
<img src=btree_merge1.png>
<p><i>Merge of leaf nodes 3 and 4 using a series of atomic page writes</i>
</center>

The figure above shows the atomic operations used to merge two pages when
they become collectively too empty. Specifically:

  #  <p>The initial state of the b-tree structure. Data is removed from 
     node 4, and the client determines it should be merged with node 3.
  #  <p>Node 4 is written first. This is a "flags only write" to set the
     SCHEDULED_FOR_EVICTION flag on the page. Once the SCHEDULED_FOR_EVICTION
     flag is set, clients must respect two rules related to the page: 
     <ul>
       <li> No client may follow a pointer to a child page once the
            SCHEDULED_FOR_EVICTION flag has been set. It must behave as if
            the pointer and its accompanying split-key have already been
            removed from the parent, and treat the flagged node and its
            left-hand sibling as a super node.
       <li> The page may not be written to again. The next thing that happens
            to the page will be that it is removed from the tree.
     </ul>
  #  A new version of node 3 is written, containing all the keys that 
     previously resided on nodes 3 and 4. Its sibling pointer is set to
     point to node 5.
  #  <p>Node 1 is updated to remove the split-key and pointer to node 4.

The third operation on the structure of a tree redistributes keys between
two adjacent nodes. This is similar to the merge shown above, except that
after step (2) a new page is allocated containing half of the keys from
the two pages over which keys are being redistributed. Then, in step (3),
instead of simply removing the right-hand peer, it is replaced by the
new node. The following figure illustrates:

<center>
<img src=btree_redistribute1.png>
<p><i>Redistribution of keys between nodes 3 and 4 using atomic operations</i>
</center>

Support for this operation not an optimization to keep the tree balanced. It
is also required to deal with the scenario where a node's left-hand peer is
written to at the same time as the node's SCHEDULED_FOR_EVICTION flag is set
(i.e. if node 3 is written concurrently with step (2) of the "merge of leaf
nodes" figure above). This might mean that the keys on the node and its peer
cannot be stored on a single page. In this case a writer must redistribute
keys instead of merging nodes.

A complication that can be caused by merge and redistribute operations is 
that super-nodes consisting of more than two pages can be created. This 
occurs when two or more adjacent peers are concurrently marked as 
SCHEDULED_FOR_EVICTION. 

<h2>Linked-List Structures</h2>

Another way to look at this kind of tree structure is as a cascade of linked
lists of pages. All data records are stored in the linked list of page leaves.
The shorter lists of internal nodes exist only to index this list of leaves.
The parent linked-list contains pointers to a subset of the pages in the
child-list of leaves.

That the parent list contains entries for a subset of child list pages,
instead of an entry for every child node as a b-tree does, does not cause
problems, as readers navigate to missing child nodes using the right-peer
pointers. Summary:

  *  When a new node is added to a child linked-list, the parent list is
     updated after the child list.

  *  When a node is removed from a child linked-list, the parent list is 
     updated before the child list (effectively - by setting a page-map
     flag on the child entry).

  *  Once a parent list entry has been written, it is "never"
     modified. The logical page number is forever associated in the parent list
     with the key it is assigned when first written. The parent entry will be
     removed if the logical page is removed from the child list altogether, but
     it is never modified.

  *  Keys are never redistributed between nodes as they are in a normal
     b-tree. Instead, all nodes except the leftmost being rebalanced are
     deleted and a new nodes added in their place.

In the above, "never" means for a very long time. In practice, logical page
numbers <a href=#pageid_management>are reused</a>, but not until it is
guaranteed that there exist no clients that may be confused by the reuse.

<center>
<img src=list1.png>
<p><i>Tree structure with missing pointers
</center>

A reader does not need to use any memory barriers after beginning its 
transaction, even if the database is being concurrently written by one or 
more writers. All that is required is a memory barrier at the start of the
transaction - to ensure that the reader does not see an old version of a
page that was overwritten even before the read-transaction was started.

Consider first a reader iterating through the linked list of leaf nodes.
Updates that merely write a new key to an existing page obviously do
not disrupt a reader. It doesn't matter if a reader sees an older or newer
version of the page - if it sees a version of the page written after the
read transaction started, new keys are ignored.

<center>
<img src=listop1.png>
<p><i>Write Operations on linked-list of leaves
</center>

It also makes no difference if a write requires adding or deleting a node
from the linked-list, or redistributing keys between a pair of nodes. The
figure above shows the three operations that may change the structure of
the list:

  *  Page 2 becomes overfull and its keys redistributed between a new
     version of page 2 and new page, page 10.
  *  Pages 4 and 5 become collectively underfull and all keys transfered
     to a new version of page 4. Page 5 is removed from the list.
  *  The keys on pages 7 and 8 are redistributed to a new version of page
     7 and new page, page 11. Page 8 is removed from the list.

The figure above shows that it doesn't matter which version of pages 2, 4 
or 7 the reader reads, as in all cases it ends up iterating through a linked
list that contains all keys of interest. It would be a problem if, in the
third case, keys from pages 7 and 8 were redistributed between new versions
of both pages 7 and 8. But they are not.

When a leaf page is added to the list, a key/pointer pair is added to the
parent list. The key is a copy of the smallest key on the leaf page, and
the "pointer" the logical page number of the new leaf. Once this key/pointer
pair has been added to the parent list, it is never modified. It may be
removed, but no other key will be associated with the pointer value until
the end of time (or, in practice, until it is <a href=#pageid_management>safe to
recycle page numbers</a>). As a result, no memory barriers are required when
following a pointer between lists either. The key/pointer pair is valid for
all version of the linked page, and all versions of the linked page are valid
for the purposes of finding keys that existed at the start of the transaction.

The only exception (which applies to pointers read from parent or peer pages)
is if no version of the linked page is found at all. For example, in the figure
above, a client may follow a pointer from the parent list to page 11. However,
assuming that page 11 was added to the list after the transaction was started,
it is possible that from the point of view of the reader the page-map entry for
logical page 11 has not yet been written.  In this case the reader sees a
physical page number of 0 (i.e. "unused logical page"). When this is
encountered, the reader must use a memory barrier and then reread the page-map
value. Because writers always update the parent list after the child list, the
page-map must hold a valid value following the memory barrier.

<h2 id=pageid_management>Page Id Management</h2>

The discussions above assume that there is an infinite supply of logical
and physical page ids, and so values never have to be reused. In practice,
both are 32-bit quantities, and so unused values must be eventually recycled.
Logical page ids can be reused immediately. Since pages will often be
accessed using mmap(), physical page ids may not be reused after after the 
last reader that may be using the page has closed its transaction.

The database header container two 4-byte fields - the largest logical and
physical ids allocated so far.

One bit of each 8-byte slot in the page array is reserved for marking the
corresponding physical page as in use. If this bit - PHYSICAL_IN_USE - is
set, the page is being used, otherwise it is free. For logical pages, the
page is free only if the mapped physical page id is 0.

<verbatim>
  /* Test if logical page l_pgno is free */
  bLogicalFree = (aPageMap[l_pgno] & 0xFFFFFFFF)==0;

  /* Test if physical page p_pgno is free */
  bPhysicalFree = (aPageMap[p_pgno] & PHYSICAL_IN_USE)==0;
</verbatim>

Obviously it would be cumbersome for a writer to search the array each 
time a new logical or physical page id is required. Instead, free ids 
are managed by a heap memory data structure protected by an ordinary mutex. 
On startup, the process will scan the entire page-map array to construct
the in-memory record of free ids. This is non-trivial, a database that
uses the maximum 2^32 pages contains 32GB of page-map data. But even with a
cold cache my aging but quite functional workstation (i7, 560 MB/s SATA3 SSD)
can read and calculate the md5sum of that amount of data in under 60 seconds, 
so this might not be as burdonsome as it sounds.

We can avoid contention for the mutex becoming a problem by allowing writers
to hold logical and physical ids such that they can be allocated from and 
returned to the central, mutex-protected, data structure in batches.

<h1 id=replication>Replication and other Integration Issues</h1>

<i> In which it is discussed how this fits in with streaming replication
and bootstrapping a new node</i>

<h1 id=robustness>Process, Power and Operating System Failures</h1>

<h2> Process Failures </h2>

The database file is always in a valid state, so does not require any
repair to recover from process failure. The log directory is scanned for
valid transactions and any found are written to the database. This ensures
that a process failure does not leave the database containing partially
committed transactions. Additionally, the entire page-map is scanned and,
based on the PHYSICAL_IN_USE and page-map values, the heap memory data
structure used to manage free logical and physical page ids is recreated.

<h2> Power/OS Failures </h2>

Recovery from power or OS failure (hereafter "system failure") is a slow
process, as it requires scanning the entire database file not once, but twice.

Each page written to the database contains a 28 byte header:

  *  The transaction id of the transaction writing the page. (8 bytes)
  *  A checksum based on the contents of the page and the checksum of
     the previous page - if any - in the transaction. (8 bytes)
  *  The logical page number of the page. (4 bytes)
  *  A pointer (physical page id) to the previous page written by the 
     transaction, if any. (4 bytes)
  *  If the page is the last page written by the transaction, a flag to
     indicate so. Also, a field to indicate the largest transaction id
     of a page clobbered (or traversed?) by the writer while writing keys, if
     it is larger than the current transaction. (4 byte)

When recovering from a system failure, it is assumed that the page-map is
completely untrustworthy. Recovering the database is to repopulate the 
page-map to contain the newest possible version of the database for which 
all valid pages have survived the catastrophe. This is done by first
determining a transaction id to recover up to. This transaction id
must meet two criteria:

  *  All pages for all transactions with transaction ids less than
     or equal to the recovery id must have made it to disk, and
  *  No transaction with a transaction id less than or equal to the
     recovery id may have overwritten a page written by a transaction 
     with a transaction id larger than the recovery id.

The second criteria above is necessary because transactions may have
been written to the db out of order - and so it is possible that
transaction N has written keys to a page that was linked into the tree
structure until by transaction N+1. In this case it is not possible
to use N as a recovery id.

If physical pages are never reused, this means that the recovery id
is the largest transaction id for which all pages of itself and
all earlier transactions can be found with valid checksums in the
database. If we require that pages written by a single transaction
have increasing physical page ids, the recovery id can be determined 
and the page-map largely reconstructed in a single pass of the database 
file.

Not completely though, as the flags stored in the page-map,
SCHEDULED_FOR_EVICTION, PHYSICAL_IN_USE and NODE_HAS_SPLIT, are still 
not set as required. Furthermore, in the case where a logical page id has 
been evicted from the tree, the page-map may still contain a non-zero value.
All of this is fixed by iterating through the entire tree structure, visiting
each page, including overflow pages.

Of course, this only works if old physical pages are never reused. In order
to facilitate the reuse of old pages without breaking recovery from system 
failure, the database file is periodically synced to disk. More specifically:

  *  The client waits until all existing transactions have finished committing,
     while preventing new transactions from beginning to write keys to the
     database. This ensures that the transaction id of the last transaction
     committed may be used as a recovery id.
  *  The db file is synced to disk.
  *  One of two 4KiB blocks at the start of the database file is updated
     with, amongst other things, the transaction id of the last transaction
     written to the db before the sync.
  *  The db file is synced to disk again.

When recovering from system failure, the client checks each of the two
4KiB blocks for the newest guaranteed recovery id. During the first scan
of the database file it then assumes all transaction up to and including
this id were successfully committed.

Once a guaranteed recovery id has been synced to disk, physical pages
made free by transactions up to and including this transaction id may
be reused.

<h1 id=embedded>Embedded Deployment</h1>


