

  *  [#system_overview|System Overview]
  *  [#transaction    |Transaction Layer Details]
  *  [#btree          |Btree Layer Details]
  *  [#replication    |Replication and other Integration Issues]
  *  [#robustness     |Process, Power and Operating System Failures]
  *  [#embedded       |Embedded Deployment]

A high-concurrency, optimistic row-level-locking backend for SQLite that
more or less plugs in to the btree.h interface. All clients must be within a
single process. Databases are robust in the face of process crashes.

The database may also be configured to be robust in the face of operating
system crashes and power failures. Recovering from such a crash requires 
scanning the entire database file. [#robustness|See below] for 
details.

Suitable for use in a server system such as BedrockDB.

There is also a [./fileformat.wiki | file format document].

<h1 id=system_overview>System Overview</h1>

The system is described here in two parts - the b-tree layer and the 
transaction layer. The b-tree layer differs from the SQLite b-tree 
in that:

  *  No direct support for transactions, sub-transactions or transaction
     rollback is provided. Each key operation (insert/update/delete) is
     immediately visible to all other database users.
  *  Concurrent writers are supported.

MVCC snapshots for readers are supported using the same technique as Postgres:

  *  Appended to each key inserted into the b-tree is a transaction id.
  *  Each key has an optional field for the transaction id of the transaction
     that deletes it from the database.
  *  Transaction ids are 56-bit monotonically increasing integers.
  *  To read the newest available stable snapshot, readers need to know the 
     largest committed transaction id for which all previous transactions
     are completely committed. They may then selectively ignore database
     entries based on the created and deleted transaction ids associated with
     each key.
  *  Writers remove entries and transaction id fields that are no longer
     required lazily, when the page is updated for some other reason.
     To do this they need to know the oldest transaction id that may
     still be being used by a reader as a snapshot id as described in
     the previous bullet point.

A writer does not write directly to the b-tree structures when executing
DML/DDL statements. Instead, new keys are accumulated in private memory
structures until the transaction is ready to commit. Also accumulated are
a list of keys and key-ranges read from the database as part of the
transaction.

The transaction queue contains an entry for each transactions that has
either been committed, or is being committed, sorted in order of transaction
id. A transaction is not added to the transaction queue or assigned a
transaction id until it is certain that it will be committed - that it does 
not conflict with any other transaction. Associated with each transaction
queue entry is the set of database keys that the transaction writes to.

When a writer wishes to commit a transaction it iterates through the 
transaction queue, starting with the entry with a transaction id
one greater than the snapshot id it used while reading the database,
in ascending transaction id order, checking for conflicts. A conflict
is found if the writer's transaction reads a key or range of keys that
was modified by a transaction already in the queue. If the writer
reaches the head of the transaction queue without finding a conflict,
it adds its own transaction to the end of the queue and in doing so
assigns itself a transaction id. It then writes its keys to the database.

<h1 id=transaction>Transaction Layer Details</h1>

This section describes roughly the transaction layer - the layer that sits
between SQLite and the b-tree described in the sections below. In general
the data structures described here require a mutex to modify, but may be
read safely without one. There are two mutexes for each database:

  *  M1: Mutex taken when adding an entry to the end of the transaction queue.
  *  M2: Mutex taken when a transaction has finished committing (after all
     keys have been written to the b-tree layer).

<b> Readers: </b>

There is a single 64-bit integer accessed using atomic store/get instructions
that contains the newest available snapshot id (the transaction id for which
all previous transactions have been completely committed). There are also two
64-bit locking-slots (or arrays of locking slots). A locking-slot is similar to
the locking slots used by SQLite's wal mode, except that in this case the lower
56-bits of the slot are used for the snapshot id and the remaining 8 for the
number of clients currently holding a lock on the slot. And so a single CAS
instruction is used to take or release a lock. The value of the newest
available snapshot field, as well as the transaction ids stored in the
locking-slots, are set by writers holding the M2 mutex.

More specifically, to open a transaction (read or write), a client must:

  1.  Read the value of both locking-slots.
  2.  Use a CAS instruction to increment the lock-count of the locking-slot
      containing the largest snapshot id. If this fails, goto 1.
  3.  Read the value of the "newest snapshot" field. This should always be
      equal to or greater than the locking-slot snapshot id.

It may be that with a sufficiently high number of clients opening and closing
transactions, contention when writing the two locking-slots might slow things
down. Or 256 or more clients require a lock on the same slot. In this case each
locking-slot will be replaced with an array of locking-slots, spaced out in
memory to avoid using the same cache-line for any two.

<b> Writers: </b>

Each writer has its own log file, and all log files are stored in a single
directory. After a writer assigns a transaction id to a transaction (once
it is sure to be committed), a record of all keys in the transaction, along
with the transaction id, is written to the log file. Once the transaction
has been completely committed, the record within the log file is marked
as invalid. 

These per-writer logs are used for process crash recovery only. On startup, 
all logs in the log-directory are scanned for transactions that were partially
committed when the crash occurred. The transactions are played forward and
the log files deleted as part of startup.

The transaction queue, the list of transactions in order of transaction
id, is an ordinary singly linked-list in heap-memory. Associated with each
of the locking-slot arrays is a pointer to the corresponding element of
the transaction queue (i.e. the element with a transaction id identical
to the locking-slots snapshot id). When a writer needs to iterate through
the transaction queue, it always starts with the pointer associated with
its locking slot.

A writer therefore proceeds as follows:

  #  Opens a read transaction as described above. During the transaction
     new keys and key-deletes are stored in memory, not written into the
     database. Also recorded are the ranges of keys read from the database
     by the transaction.
  #  Iterate through the transaction queue, checking if the current
     transaction conflicts with a transaction with an id greater than
     the snapshot id opened in (1). If a conflict is found, return the 
     error to the user.
  #  Grab mutex M1. Append an entry for the current transaction to the
     transaction queue and assign the next transaction id to it. Release
     M1. Or, if when M1 is obtained some transaction that was not checked
     for conflicts in (2) has already been appended to the list, drop mutex
     M1 and goto 2.
  #  Write an entry for the transaction to the writers log file.
  #  Write all the transactions key and delete-keys to the database.
  #  Grab mutex M2 and mark the transaction queue entry as fully committed
     and update the newest-available-snapshot field if required. Also,
     if possible, update the locking-slot values and the transaction queue
     pointers (see below).

The transaction layer data structure can be summarized as follows:

<verbatim>
    struct TransactionLayer {
      Mutex M1;
      Mutex M2;
      u64 aLockingSlot1[ARRAY_SIZE];
      u64 aLockingSlot2[ARRAY_SIZE];
      u64 iNewestAvailableSnapshot;
      TransactionQueueEntry *p1;
      TransactionQueueEntry *p2;
    };
</verbatim>

Not shown above is the "largest transaction id allocated so far" field. This
is part of the header of the database itself.

After obtaining mutex M2 once all keys have been written to the database,
a writer check if it is possible to update a locking-slot value and
transaction queue pointer. It is always the locking-slot with the smaller
value that is updated. The update takes place iff (a) there are no readers
holding any locks on the locking-slot, and (b) the newest-available-snapshot-id
is greater than the value stored in the other locking slot (as it is important
that the locking slots always contain different values). The update proceeds as
follows:

  #  Each value in the locking-slot array is set to 0. This ensures that no
     reader locks it.
  #  The transaction-queue pointer is advanced to point to the entry with
     a transaction id equal to the current newest-available-snapshot-id
     value. All transaction queue entries between the current pointer and
     the pointer associated with the other locking slot can be deleted
     at this point.
  #  Each value in the locking-slot array is set to the
     newest-available-snapshot-id value.



<h1 id=btree>Btree Layer Details </h1>

The b-tree is an ordinary single-process, multi-threaded, COW, (mostly)
lock-free b-tree implementation. As with SQLite, multiple tree structures 
exist within a single file - one for each database table and index.

<a href=blink.wiki>Now described here.</a>

<h1 id=replication>Replication and other Integration Issues</h1>

<i> In which it is discussed how this fits in with streaming replication
and bootstrapping a new node</i>

<h1 id=robustness>Process, Power and Operating System Failures</h1>

<h2> Process Failures </h2>

The database file is always in a valid state, so does not require any
repair to recover from process failure. The log directory is scanned for
valid transactions and any found are written to the database. This ensures
that a process failure does not leave the database containing partially
committed transactions. Additionally, the entire page-map is scanned and,
based on the PHYSICAL_IN_USE and page-map values, the heap memory data
structure used to manage free logical and physical page ids is recreated.

<h2> Power/OS Failures </h2>

Recovery from power or OS failure (hereafter "system failure") is a slow
process, as it requires scanning the entire database file not once, but twice.

Each page written to the database contains a 28 byte header:

  *  The transaction id of the transaction writing the page. (8 bytes)
  *  A checksum based on the contents of the page and the checksum of
     the previous page - if any - in the transaction. (8 bytes)
  *  The logical page number of the page. (4 bytes)
  *  A pointer (physical page id) to the previous page written by the 
     transaction, if any. (4 bytes)
  *  If the page is the last page written by the transaction, a flag to
     indicate so. Also, a field to indicate the largest transaction id
     of a page clobbered (or traversed?) by the writer while writing keys, if
     it is larger than the current transaction. (4 byte)

When recovering from a system failure, it is assumed that the page-map is
completely untrustworthy. Recovering the database is to repopulate the 
page-map to contain the newest possible version of the database for which 
all valid pages have survived the catastrophe. This is done by first
determining a transaction id to recover up to. This transaction id
must meet two criteria:

  *  All pages for all transactions with transaction ids less than
     or equal to the recovery id must have made it to disk, and
  *  No transaction with a transaction id less than or equal to the
     recovery id may have overwritten a page written by a transaction 
     with a transaction id larger than the recovery id.

The second criteria above is necessary because transactions may have
been written to the db out of order - and so it is possible that
transaction N has written keys to a page that was linked into the tree
structure until by transaction N+1. In this case it is not possible
to use N as a recovery id.

If physical pages are never reused, this means that the recovery id
is the largest transaction id for which all pages of itself and
all earlier transactions can be found with valid checksums in the
database. If we require that pages written by a single transaction
have increasing physical page ids, the recovery id can be determined 
and the page-map largely reconstructed in a single pass of the database 
file.

Not completely though, as the flags stored in the page-map,
SCHEDULED_FOR_EVICTION, PHYSICAL_IN_USE and NODE_HAS_SPLIT, are still 
not set as required. Furthermore, in the case where a logical page id has 
been evicted from the tree, the page-map may still contain a non-zero value.
All of this is fixed by iterating through the entire tree structure, visiting
each page, including overflow pages.

Of course, this only works if old physical pages are never reused. In order
to facilitate the reuse of old pages without breaking recovery from system 
failure, the database file is periodically synced to disk. More specifically:

  *  The client waits until all existing transactions have finished committing,
     while preventing new transactions from beginning to write keys to the
     database. This ensures that the transaction id of the last transaction
     committed may be used as a recovery id.
  *  The db file is synced to disk.
  *  One of two 4KiB blocks at the start of the database file is updated
     with, amongst other things, the transaction id of the last transaction
     written to the db before the sync.
  *  The db file is synced to disk again.

When recovering from system failure, the client checks each of the two
4KiB blocks for the newest guaranteed recovery id. During the first scan
of the database file it then assumes all transaction up to and including
this id were successfully committed.

Once a guaranteed recovery id has been synced to disk, physical pages
made free by transactions up to and including this transaction id may
be reused.

<h1 id=embedded>Embedded Deployment</h1>


