

  *  [#system_overview|1. System Overview]
  *  [#concurrency    |2. Concurrency Model]
    <ul>
      <li> [#concurrency_bg    |2.1. Background]
      <li> [#concurrency_hct   |2.2. Overview of HC-Tree Concurrency]
      <li> [#concurrency_notes |2.3. Notes on HC-Tree Concurrency]
    </ul>
  *  [#replication    |3. Replication and other Integration Issues]

A high-concurrency, optimistic row-level-locking backend for SQLite that
more or less plugs in to the btree.h interface. Goals are:

  *  To run significantly faster than stock SQLite in all important cases, and
  *  To allow an order of magnitude more write concurrency than stock SQLite.

All clients must be within a single process. Databases are robust in the face
of process crashes. At the cost of extra writes and file-system syncs, the
database may also be configured to be robust in the face of operating system
crashes and power failures.

Suitable for use in a server system such as BedrockDB.

There is also a document describing the concurrent [./blink.wiki|b-link tree]
used to store data persistently in the file-system, and a
[./fileformat.wiki|file-format document] describing the structure of the 
various types of files.

<h1 id=system_overview>1. System Overview</h1>

The system is built around the b-tree backend [./blink.wiki|described here].
The b-tree layer differs from the SQLite b-tree in that:

  *  No direct support for transaction isolation or rollback is provided. Each
     key operation (insert/update/delete) is immediately visible to other
     database users.
  *  Concurrent writers are supported.

MVCC snapshots for readers are supported using the same technique as Postgres:

  *  Along with each key or delete-key inserted into the b-tree is a
     transaction id.  
  *  Each key has an optional field for the transaction id (TID) of the
     transaction that deletes it from the database.
  *  If there are multiple versions of the key in the database, they are
     linked into a list so that it is possible to navigate from a new
     key to all older versions.
  *  Readers carry with them a record of the transactions that were committed
     at the time when the read transaction was started and the snapshot opened.
  *  By selectively ignoring or using older versions of records based on
     the associated TIDs, readers obtain a consistent view of the database
     snapshot on which their read transaction is opened.

A writer does not write directly to the b-tree structures when executing
DML/DDL statements. Instead, new keys are accumulated in private memory
structures until the transaction is ready to commit. Also accumulated are
a list of keys and key-ranges read from the database as part of the
transaction.

Writers use the same rule as the SQLite BEGIN CONCURRENT branch, modified
to work with row-level locks instead of page-level locks, to determine
whether or not a transaction may be safely committed or not: The transaction
may be committed if none of the keys or key ranges that it read (or wrote, each
write is also implicitly a read) have been modified since the transactions
snapshot. This is described in <a href=#concurrency_hct>more detail</a> below.

  


<h1 id=concurrency>2. Concurrency Model</h1>

<h2 id=concurrency_bg>2.1. Background</h2>

Until surprisingly recently, concurrency in SQL database systems was much
simpler. Transactions were in most cases allowed to proceed concurrently so
long as they did not write to the same rows, or cause database constraint
violations. This is often termed <b>snapshot isolation</b>. Although this is 
simpler to implement and imposes relatively little overhead, it turns out to be
quite difficult to use, as concurrent combinations of transactions that
function correctly when run independently may malfunction.

<pre>
    CREATE TABLE marbles(id INTEGER PRIMARY KEY, color);
    INSERT INTO marbles VALUES(1, 'white'), (2, 'black'), (3, 'white'), (4, 'black');
</pre>

And two transactions, one that changes the color of all marbles to white, and
one that changes all marbles to black:

<pre>
    -- Transaction 1:
    UPDATE marbles SET color='white' WHERE color='black';
</pre>

<pre>
    -- Transaction 2:
    UPDATE marbles SET color='black' WHERE color='white';
</pre>

Intuitively, after running both of the transactions above concurrently, there
are two possible outcomes - either all of the marbles are white or all of the
marbles are black. However, with snapshot isolation there is a third
possibility - that the color of each marble has been changed, leaving half
white and half black.

Modern systems implement <b>serializable</b> isolation, which does not allow
for this third possibility. Under serializable isolation, a set of
transactions may only be committed if the same transactions may be executed
in some serial order producing results consistent with the final and all
visible interim states of the database produced by concurrent execution. The
usual way to model this is as a graph, where each node is a transaction, and
each directly edge represents a dependency. There are two types of dependency
between transactions:

  *  a <b>dependency</b> which occurs when transaction B reads data written
     by transaction A. In this case transaction B must occur after transaction
     A in the serialization order.

  *  an <b>anti-dependency</b> which occurs when transaction A reads data
     that is overwritten by transaction B. In this case transaction A must
     occur before transaction B in the serialization order.

when a transaction is to be committed, it is added to the directed graph. If
this creates a cycle in the graph, the transaction may not be committed.

One quirk in this is that read-only transactions must also be added to the
graph and be validated before "committing". Results returned by a
read-transaction that does not pass validation must be considered suspect.
For example, consider the following two transactions:

<pre>
    -- T1:
    UPDATE marbles SET color='white';
</pre>

<pre>
    -- T2:
    INSERT INTO marbles VALUES(NULL, 'black');
</pre>

The final state of the database is that it contains one black marble, and the
rest are white. This implies that there is an anti-dependency between T1 and T2
- T1 read data that was later overwritten by T2, so T1 must occur before T2 in
the serialization order:

<verbatim type="pikchr center">
circle "T1" ; arrow ; circle "T2"
</verbatim>

In the above diagram, the arrow runs in the direction of time. The graph
indicates that T2 must occur after T1 in the serialization order.

Say there is also a read-only transaction running:
<pre>
  -- T3:
  SELECT * FROM marbles;
</pre>

If, by some quirk of the system, saw the effects of T2 but not T1 (a mix of
black and white marbles plus the new black marble added by T2), then it would
have a dependency on T2 but an anti-dependency on T1:

<verbatim type="pikchr center">
T3: circle "T3"
T1: circle "T1" at T3 + (-0.5,0.7) ; arrow ; T2: circle "T2"
arrow from T2.sw to T3.ne
arrow from T3.nw to T1.se
</verbatim>

In this case, one of more of T1, T2 or T3 would have to be rolled back. Even
though the both the final state and interim state of the database observed
by T3 are possible outcomes of executing T1 and T2 in serial, they are not
consistent with each other and therefore a violation of serializable
isolation.

A full implementation of such a graph is considered too expensive in 
practice. And so all current systems implement some compromise that allows
for some number of false-positive conflict detections.

<h2 id=concurrency_hct>2.2. Overview of HC-Tree Concurrency</h2>

A write transaction writes no data into the database while executing DDL
or DML statements. Instead, new database entries and delete keys are
accumulated in a private, in-memory, data structure. Queries run within
the transaction merge data from the database and in-memory tree structures
to return to the caller. Also accumulated in memory is a record of the
keys and key-ranges read from the database as part of the transaction.

Associated with each record or delete-key in the database is a transaction ID
(TID). A TID is a 56-bit integer assigned to transactions in increasing order
as described below. A TID's only purpose is as a key to use with the
transaction-map. The transaction-map maps each TID to one of four types
of values:

  *  Special value WRITING. This means that keys for the transaction 
     are being written into the database.
  *  Special value ROLLBACK. This means that keys are currently being
     removed from the database (replaced with their previous versions). This
     happens after a conflict is detected and it is determined that the
     transaction will not be committed.
  *  A commit ID (CID) value with the VALIDATING flag set.
  *  A commit ID (CID) value.

Commit IDs (CIDs) are also 56-bit integers assigned to transactions in
increasing order. A database snapshot is defined by a commit id <i>C</i> - all
keys in the file with with TIDs mapped to CIDs less than or equal to <i>C</i>
are part of the snapshot and should be read, all other keys should be ignored.

A commit proceeds as follows:

  #  <p>The transaction is assigned a transaction ID (TID). The transaction 
     map entry is set to WRITING.
  #  <p>The transaction writes its set of new keys and delete-keys into the
     database. The transaction id assigned in the previous step is associated
     with each new database entry. If the writer finds that it must clobber
     a version of a key with a TID that indicates that the key being clobbered
     was written after the writers snapshot, the transaction is deemed to
     conflict and any changes rolled back.
  #  <p>The transaction is assigned a commit ID (CID). The transaction map
     entry is set to contain the CID with the VALIDATING flag set.
  #  <p>The transaction queries the database for the keys and key ranges that
     it queried for while it was running. If it encounters any keys with
     TIDs mapped to CID values greater than the CID of the database snapshot
     that the transaction was prepared against, but less than the CID assigned
     in step 3, the transaction is deemed to conflict and all changes rolled
     back.
  #  The VALIDATING flag is cleared in the transaction map. The transaction
     is now committed and its keys may be read by readers.

To rollback, the database client:

  #  <p>Sets the transaction map entry for the transaction to ROLLBACK.
  #  <p>Runs through all keys already written to the database, restoring the 
     original values.
  #  <p>Sets the transaction map entry for the transaction to the CID
     value with the ROLLED_BACK flag set. Any reader that sees a CID with
     the ROLLED_BACK flag set should reseek the cursor to find the rewritten
     entry.

This scheme means that a transaction may finish committing before all
transactions with smaller CID values have. If a reader is allowed to read the
database in this state, it may encounter data anomalies similar to that
described at the end of the previous section. Although each database snapshot
is possible in and of itself, a single reader might see a sequence of database
snapshots that would not be possible if transactions were truly serialized.

One solution might be to have new readers read from the database snapshot 
defined by the largest CID for which the transaction and all transactions
earlier in the serialization order (those with smaller CID values) have
already finished committing, ignoring some recently committed transactions. 
But this means that a single database connection might read from a snapshot
that does not contain the effects of its own previous write transactions, 
which is not tenable.

Instead, a new reader reads from the snapshot defined by the largest CID
that has finished committing so far. If it encounters a key with a TID
mapped to a CID with the VALIDATING flag set, the reader spinlocks on
the transaction-map entry until it is either set to ROLLBACK or until the
VALIDATING flag is cleared. The reader then proceeds - reading either the
key or the previous version (if any), depending on whether or not the
transaction was committed or rolled back.

<h2 id=concurrency_notes>2.3. Notes on HC-Tree Concurrency</h2>

  *  The system <b>does not have to store the entire transaction map</b>.
     Instead, it can be compressed by storing a transaction id Tmin for
     which there are no readers reading from snapshots old enough to
     exclude transaction Tmin or any transaction with a TID smaller than
     Tmin from the snapshot. Then all entries for TIDs Tmin and smaller
     may be discarded.

  *  <b>The validation phase can be optimized by using page level checks
     before falling back to range and key checks</b>. When a client records
     the list of keys and ranges that it reads during the transaction,
     it also records the 
     <a href=blink.wiki#overview>page-map value</a> for each logical page that
     it reads. When validating a key or range of keys, it first comparse the
     recorded page-map values with the current page-map value or values. If the
     page-map values have not changed, there is no need to proceed with the
     more expensive key level checks.

  *  <b>This permits less concurrency than some other schemes</b>. This is 
     largely a consequence of not wishing to validate read-only transactions.
     The technique used most successfully to increase concurrency relative
     to the scheme described above is Serializable Snapshot Isolation (SSI):
     <p> [https://wiki.postgresql.org/wiki/Serializable]
     <p>Another, similar, method is the Serial Safety Net:
     <p>[https://event.cwi.nl/damon2015/papers/damon15-wang.pdf]

<h3> 2.3.1. Increasing Concurrency </h3>

<i> In which it is discussed how to enhance the above to be equivalent 
to SSN. </i>

<h1 id=replication>3. Replication and other Integration Issues</h1>

<i> In which it is discussed how this fits in with streaming replication
and bootstrapping a new node</i>


